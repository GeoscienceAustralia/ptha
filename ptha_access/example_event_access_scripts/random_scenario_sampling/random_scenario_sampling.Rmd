# Randomly sample PTHA18 scenarios on a source-zone
---------------------------------------------------

The PTHA18 often includes thousands or tens-of-thousands of scenarios on a
source-zone. For some applications it is impractical to work with all
scenarios, but may be practical to work with a random sample of scenarios that
have similar statistical properties. 

For example, suppose we wish to conduct a probabilistic tsunami inundation
hazard assessment, which will require running computationally expensive
inundation models for every scenario. It may be impractical to do this for
every PTHA18 scenario, but feasible with a random sample containing hundreds or
thousands of scenarios. 

This tutorial shows how to randomly sample scenarios from a given source-zone
in a manner that is statistically consistent with the PTHA18. This means that
one can derive quantities of interest (such as the maximum-stage
exceedance-rate at a hazard point) from the random scenarios, and the result
will be arbitrarily close to the PTHA18 values IF the random sample is
sufficiently large. 

## Get the source-zone event data, and some maximum-stage data.
---------------------------------------------------------------

The first step is to get the scenario data for the source-zone of interest.
Here we choose to work with heterogeneous-slip scenarios from the
`kermadectonga2` source-zone. 
```{r startup}
# Get the scripts to access the PTHA18
ptha18 = new.env()
source('../../get_PTHA_results.R', local=ptha18, chdir=TRUE)
# Read all heterogeneous-slip scenario metadata (slip_type='stochastic' in PTHA18)
source_zone = 'kermadectonga2'
kt2_scenarios = ptha18$get_source_zone_events_data(source_zone,  slip_type='stochastic')
```

To illustrate how we can use the random scenarios, it is useful to have the corresponding
tsunami maximum-stage data at a point of interest. Herein we choose a point just east of Tonga,
which is over the `kermadectonga2` source-zone.

```{r peakStageTonga}
event_peak_stage_at_refpoint = ptha18$get_peak_stage_at_point_for_each_event(
    target_point = c(185.1239, -21.0888), # Known location of PTHA18 hazard point
    slip_type='stochastic',
    all_source_names=source_zone)

# Convenient shorthand
event_peak_stage = event_peak_stage_at_refpoint$kermadectonga2$max_stage
```

## Random scenario sampling, stratified by magnitude
----------------------------------------------------

Our simplest random scenario sampling algorithm proceeds as follows
* Group the scenarios by magnitude
* For each magnitude, sample a given number of scenarios with replacement, with the chance of sampling each scenario proportional to its conditional probability.

The function which does this only requires knowledge of the scenario magnitudes, and the scenario rates. From these variables the function will internally compute scenario conditional probability for each unique magnitude value (which ranges from 7.2, 7.3, ... 9.6, 9.7, 9.8 in PTHA18). We also need to specify the number of scenarios to sample for each magnitude - herein a constant (12) is used, although in general it can vary with magnitude.
```{r randomSampling1}
# Convenient shorthand for the magnitudes and rates in the event table
event_Mw = kt2_scenarios$events$Mw
event_rates = kt2_scenarios$events$rate_annual

# Make a reproducible random seed to make the code reproducible (this is
# optional)
set.seed(123)

# Make the random scenarios
random_scenarios_simple = ptha18$randomly_sample_scenarios_by_Mw_and_rate(
    event_rates=event_rates,
    event_Mw=event_Mw,
    samples_per_Mw=function(Mw){ 12 }, # Number of samples for each Mw
    mw_limits=c(7.15, 9.85) # Optionally limit the mw range of random samples
    )

```

The result is a `data.frame` containing the indices of the random scenarios `inds`,
their magnitudes, `mw`, as well as information on the scenario rates that will be discussed
further below.

```{r randomSampling1_B}
# Look at the first few rows
head(random_scenarios_simple)
```
The columns are
* `inds` is the indices of the randomly selected scenarios. This corresponds to indices in the `event_Mw` and `event_rates` variables. Because herein these are simply columns of the event table, `inds` also also correspond to rows in `kt2_scenarios$events`.
* `mw` is the scenario magnitude. This is the same as `event_Mw[random_scenarios_simple$inds]`
* `rate_with_this_mw` is the rate of ANY scenario with the same magnitude. This is the sum of `event_rates` for scenarios with the corresponding magnitude. Note THIS IS NOT THE RATE OF THE INDIVIDUAL SCENARIO!
* `importance_sampling_scenario_rates` is a nominal rate for each scenario, defined so as to retain statistical consistency with the PTHA18. In this particular case it is equal to the `rate_with_this_mw` divided by the number of scenarios with that same magnitude (12 in this case). In more complex applications we can specify an `event_importance` to bias the sampling toward scenarios of interest, and in that case its definition is more complicated, but the interpretation is similar.
* `importance_sampling_scenario_rates_self_normalised` is another nominal rate for each scenario. In this case it is identical to the previous variable. However later we will consider more complex sampling methods, using importance sampling, where it may be somewhat different (basically it can be considered as an alternative statistical estimator of the same thing).
* `importance_sampling_scenario_weights` is equal to `importance_sampling_scenario_rates` divided by `rate_with_this_mw`. Later when we do importance sampling, this corresponds to the regular importance sampling weights. 
* `importance_sampling_scenario_weights_self_normalised` is equal to `importance_sampling_scenario_rates_self_normalised` divided by `rate_with_this_mw`. Later when we do importance sampling, this corresponds to the self-normalised importance sampling weights. 

In PTHA18 some earthquake magnitudes are impossible. In this case the scenario index will
take an `NA` value, as will various other variables. We see this at the end of the current
table, for magnitudes `9.7` and `9.8`.

```{r randomSampling1_C}
# Look at the last few rows
tail(random_scenarios_simple)
```

Aside from the impossible magnitudes, we can confirm that we have 12 scenarios per magnitude, as requested.
```{r randomSampling1_nsam}
table(random_scenarios_simple$mw)
```

## Inferring tsunami exceedance-rates from the random scenario subset
----------------------------------------------------------------------

What do we mean by saying the random scenarios are statistically consistent
with the PTHA18? Here we explain this by considering the tsunami max-stage
exceedance-rates at the point offshore of Tonga.

In the full PTHA, we can compute the max-stage exceedance rates at this
point as follows:
```{r ptha18_tonga_point_exrates}
stage_seq = seq(0.1, 20, by=0.1)
stage_exrates_ptha18 = sapply(stage_seq, f<-function(x) sum(event_rates*(event_peak_stage > x)))

```

The analogous calculation using only the random sample is:
```{r ptha18_tonga_point_random}
random_scenarios_peak_stage = event_peak_stage[ random_scenarios_simple$inds ]
random_scenarios_rates = random_scenarios_simple$importance_sampling_scenario_rates
stage_exrates_random_scenarios_simple = sapply(stage_seq, 
    f<-function(x) sum(random_scenarios_rates * (random_scenarios_peak_stage > x), na.rm=TRUE))

```

While not identical, the results are quite similar at common exceedance-rates.
If we increased the number of random scenarios per magnitude from 12 to
something larger, the accuracy would improve. 
```{r ptha18_tonga_point_plot1}
# Plot it
plot(stage_seq, stage_exrates_ptha18, log='xy', t='o'); grid(col='orange')
points(stage_seq, stage_exrates_random_scenarios_simple, t='l', col='red')
```

Although in this case we could just use the full PTHA18 results, in other
situations we might be interested in the tsunami behaviour away from PTHA18
output points - for instance at an onshore site of interest. In that case we need to
re-simulate the tsunami for every scenario with a relatively costly inundation
model. While this is likely computationally prohibitive for the full set of
PTHA18 scenarios, it may be feasible for a random subset of scenarios.

## Random scenario sampling, with more scenarios at magnitudes of interest
--------------------------------------------------------------------------

The simple random sample has many scenario with low maximum-stage values, which
are not of particular interest for this study. (DEMONSTRATE WITH A HIST)

```{r scenarios_more_at_high_Mw}
# Make the random scenarios
random_scenarios_mw_weighted = ptha18$randomly_sample_scenarios_by_Mw_and_rate(
    event_rates=event_rates,
    event_Mw=event_Mw,
    samples_per_Mw=function(Mw){ round( 6 + 12 * (Mw - 7.15)/(9.65 - 7.15) ) }, # Number of samples for each Mw
    mw_limits=c(7.15, 9.85) # Optionally limit the mw range of random samples
    )

# Compute the max-stage exceedance-rates
stage_exrates_random_scenarios_mw_weighted = sapply(stage_seq, 
    f<-function(x){
        sum(random_scenarios_mw_weighted$importance_sampling_scenario_rates * 
            (event_peak_stage[random_scenarios_mw_weighted$inds] > x), na.rm=TRUE)
    })
```

This one is not that much better in terms of the concentration at higher max-stage
values.

## Random scenario sampling, using importance sampling to emphasise higher max-stages
-------------------------------------------------------------------------------------

```{r scenarios_stage_biased}
# Make the random scenarios
POW = 1 # 1.5 is quite good?
random_scenarios_stage_mw_weighted = ptha18$randomly_sample_scenarios_by_Mw_and_rate(
    event_rates=event_rates,
    event_Mw=event_Mw,
    event_importance = event_peak_stage**POW,
    samples_per_Mw=function(Mw){ round( 6 + 12 * (Mw - 7.15)/(9.65 - 7.15) ) }, # Number of samples for each Mw
    mw_limits=c(7.15, 9.85) # Optionally limit the mw range of random samples
    )

# Compute the max-stage exceedance-rates
stage_exrates_random_scenarios_stage_mw_weighted = sapply(stage_seq, 
    f<-function(x){
        sum(random_scenarios_stage_mw_weighted$importance_sampling_scenario_rates * 
            (event_peak_stage[random_scenarios_stage_mw_weighted$inds] > x), na.rm=TRUE)
    })

# As above, using the self-normalised importance sampling approach
stage_exrates_random_scenarios_stage_mw_weighted_B = sapply(stage_seq, 
    f<-function(x){
        sum(random_scenarios_stage_mw_weighted$importance_sampling_scenario_rates_self_normalised * 
            (event_peak_stage[random_scenarios_stage_mw_weighted$inds] > x), na.rm=TRUE)
    })
```

## Comparison of all approaches
-------------------------------

Emphasise that increasing the sample size is a good idea. Also tests, like
checking generality at nearby points, and splitting scenarios into groups (more complex?), 
and randomisation tests.

```{r plotthis}
## Plot it
#plot(stage_seq, stage_exrates_ptha18, log='xy', t='l', lwd=2, 
#     xlim=c(0.1, 10), ylim=c(1e-04, 1e-01)) 
#points(stage_seq, stage_exrates_random_scenarios_simple, t='l', col='red')
#points(stage_seq, stage_exrates_random_scenarios_mw_weighted, t='l', col='blue')
#points(stage_seq, stage_exrates_random_scenarios_stage_mw_weighted, t='l', col='purple')
#points(stage_seq, stage_exrates_random_scenarios_stage_mw_weighted_B, t='l', col='brown')
#grid(col='orange')
#legend('bottomleft',
#    c('Original PTHA [best result]', 'Simple random sampling (12 per Mw)', 'More scenarios at higher Mw',
#      'Importance based on event_peak_stage', 'Importance based on event_peak_stage (self normalised)'),
#    col=c('black', 'red', 'blue', 'purple', 'brown'), lwd = c(2, 1, 1, 1, 1), bg='white')
```
